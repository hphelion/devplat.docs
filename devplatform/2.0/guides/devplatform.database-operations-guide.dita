<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic xml:lang="en-us" id="database_operations_guide">
  <title>HPE Helion Development Platform 2.0: Database Service Operations Guide</title><abstract><shortdesc outputclass="hdphidden">Documentation to help operate the Database service.</shortdesc></abstract>
   <body>
     <section id="expandCollapse">
       <sectiondiv outputclass="expandall">Expand All Sections</sectiondiv>
       <sectiondiv outputclass="collapseall">Collapse All Sections</sectiondiv>
     </section>
       <section>
         <p>When HA is enabled, the database service will distribute its multiple database instances across the compute hosts in those zones, ensuring that a single hardware failure in one availability zone (AZ) does not impact more than one database instance. </p>
         
         <p>Other users who request virtual machines (VMs), but do not specify a particular AZ, may have their VM instances scheduled from those AZs assigned for use by the database service.  This means that other VMs can consume compute resources from those 
             AZs used by the database service, and so the administrator will need to ensure that sufficient compute resources are available for new database service requests.</p>
         
         <p>If a compute host is not associated with an AZ, the Database Service will not be able to provision a database instance to it. As a result, the Database Service may run out of capacity even though there are compute 
             hosts with free capacity in the Helion OpenStack instance. </p>
           
         <p>When a user requests a database via the Database Service API, CLI, or horizon console, they provide the amount of OpenStack Block storage required for the database and VM instance flavor to be used.	</p>
           
         <p>Once a new database becomes active, users connect to it via the datastore command-line client, datastore tools, or libraries used by applications.  Users can also assign a floating IP address to the database, and update the security group 
             to allow for connection from outside the external network.</p>
         </section>
     <section><title outputclass="headerH">Data Durability</title>
       <sectiondiv outputclass="insideSection">

           <p>The Database service ensures database content is protected through its integration with the
               HPE Helion OpenStack Block Storage. HPE Helion Openstack offers two cinder block storage
               plugin choices that provide Highly Available RAID protected Storage volumes for use by the
               user databases:</p>
           <ul>
               <li>HPE 3PAR storage arrays</li>
               <li>StoreVirtual VSA software running on ProLiant servers that creates a virtual array using the ProLiant server internal disks.</li>
             </ul>
           <p>For more information on HPE Helion OpenStack Cinder block storage and on capacity estimation
               guidelines, see <xref
                   href="http://docs.hpcloud.com/#commercial/GA1/1.1commercial.-vsa-overview.html"
                   format="html" scope="external">Cinder Block Storage Using HPE StoreVirtual VSA</xref>.</p>
         <ul>
           <li>
             <p>For documentation on backup/restore capability for single-instance client databases, see 
             <xref href="../../2.0/database/devplatform.database-backup.html" scope="local">Back up a Database</xref>. 
             HPE Helion Development Platform (HDP) does not provide a tool or automation for establishing a backup policy and schedule. You can either use existing solutions to configure your desired policy, 
             or use “cron” type tools. Depending on how accessible your client databases are from outside of a Helion OpenStack environment, you might initiate backups from external cloud providers 
             (such as AWS), or copy backup data to outside the boundaries of the Helion OpenStack. HDP does not provide a mechanism for that within the product. There is also database replication functionality 
             for single instances, but this is not applicable for clustered databases (such as MySQL clusters).</p>
             
           </li>
           <li><p>For clustered client databases, depending on datastore type (MySQL, MongoDB, Redis, or Vertica Preview), use the respective data backup tools that are available externally. 
             Examples: mysqldump, innobackupex, mongodump.  Also, make sure you are considering whether to encrypt the data as you back it up. It is recommended that you copy and store the backup 
             in an external location or on different AZs.
           </p>
             <p>A client database will show in SHUTDOWN state in case the datastore process for it is down. This will be shown in “trove list --include-cluster”, or “trove show” command for a specific database. 
               You can implement an automated job to detect and alert based on the state of databases.
             </p>
             <p>In addition to the database state health check, you also need to monitor the availability of the load balancer for clustered databases, by implementing an automated tool to poll the 
               state of the load balancer.
             </p>
             <p>For MongoDB clusters, you also need to check the status and availability of the MongoDB cluster query router.</p>
             <p>In a DR scenario impacting the client databases, first identify the cause of the outage: whether it is load balancer or network related, or database cluster outage. In case of a load balancer issue, recover the load balancer.
             </p></li>
           <li>In case of database cluster outage, do the following:
             <ol>
               <li>Create a new database cluster</li>
               <li>Use the same data dump tool to perform a restore of the backed up data to the new database cluster</li>
               <li>Ensure your data is restored correctly and begin using the new clustered database</li>
             </ol>
           </li>
           <li><p>For documentation for High Availability (HA) capability for Trove Control Plane VMs, see <xref href="../../2.0/database/devplatform.database-architecture.htm" scope="local">
              Database Service Architecture</xref>.  </p>
             <p>For HA, the number of AZs and hardware redundancy determines your HA capability. In a single rack, all AZs will share the same power/network. The Trove Control Plane database is a 
               MySQL Galera cluster (active/active multi-master sync config). It can tolerate down to one node staying up (on a 3 AZ config, 2 node failures). In addition, control plane database is 
               automatically backed up to Swift every hour.
             </p></li>
           <li>In a Disaster Recovery (DR) scenerio, if the DBaaS control plane goes down, you need
            to redeploy DBaaS and restore the control plane database using data from the Swift
            backup. You can find the Swift backup file as follows: <ol>
              <li>Swift container name: <b>backup_container</b></li>
              <li>Swift backup file name format:
                  <b>&lt;hostname&gt;-&lt;date&gt;-backup-percona</b><p>where &lt;hostname&gt; is
                  the name of the DBaaS control plane database host. Run <codeph>nova list</codeph>
                  to get the database host name that resembles <codeph>trove-db-....</codeph>
                  prefix. </p></li>
            </ol>
          </li>
         </ul>
       </sectiondiv>
         </section>
   </body>
     <topic id="troubleshooting">
       <title>Troubleshooting</title>
       <body>
         <p>The following section contains troubleshooting information for Trove instances, such as for when a Trove node reboots unexpectedly and needs to be brought back up.</p>
         
         <!-- 
           Logging in to a Trove instance
           -->
         <section><title outputclass="headerH">Logging in to a Trove instance</title>
           <sectiondiv outputclass="insideSection">
         <p>To troubleshoot Trove issues from the command line, you must first enable SSH to Trove instances. The following procedure demonstrates how to enable SSH for a Trove instance and log in:</p>
         <ol>
           <li>Source <codeph>service.rc</codeph>:
             <codeblock>stack@eu-em2-cp1-c0-m1-mgmt:~$  . service.osrc</codeblock></li>
           <li>Get the <codeph>secid</codeph>:
             <codeblock>stack@eu-em2-cp1-c0-m1-mgmt:~$ secid=$(nova secgroup-list | awk '/check_mk/ {print $2}')
stack@eu-em2-cp1-c0-m1-mgmt:~$ echo $secid
4405b674-24e6-4f73-83bb-a8c499739454</codeblock></li>
           <li>Add access to the host
             <codeblock>stack@eu-em2-cp1-c0-m1-mgmt:~$ nova secgroup-add-rule $secid tcp 22 22 0.0.0.0/0 # access one of the hosts
+-------------+-----------+---------+-----------+--------------+
| IP Protocol | From Port | To Port | IP Range  | Source Group |
+-------------+-----------+---------+-----------+--------------+
| tcp         | 22        | 22      | 0.0.0.0/0 |              |
+-------------+-----------+---------+-----------+--------------+</codeblock></li>
           <li>Show the trove nodes:
             <codeblock>stack@eu-em2-cp1-c0-m1-mgmt:~$ nova list |grep trove-data
| b598d436-9865-4822-bfe0-ef2dc837e55e | trove-database0-5bcqh2i3khcu        | ACTIVE | -          | Running     | SVC=10.16.86.130; trove_mgmt_network=172.17.0.22                                  |
| fd73f868-3874-4791-bdce-8e9e737c53dc | trove-database1-wkkpkgc3clc4        | ACTIVE | -          | Running     | SVC=10.16.86.133; trove_mgmt_network=172.17.0.25                                  |
| 6ac79a88-2bde-4954-ac09-9395d38fce4a | trove-database2-kms5knwquu63        | ACTIVE | -          | Running     | SVC=10.16.86.131; trove_mgmt_network=172.17.0.23</codeblock></li>
           <li>Log in to the node as <codeph>ec2-user</codeph>:
             <codeblock>stack@eu-em2-cp1-c0-m1-mgmt:~$ ssh ec2-user@10.16.86.130
sudo -i</codeblock></li>
           <li>If the above fails, log in using the <codeph>dbaas-key</codeph> certificate:
             <codeblock>stack@eu-em2-cp1-c0-m1-mgmt:~$ ssh -i dbaas-key ec2-user@10.16.86.130</codeblock></li>
         </ol>
           </sectiondiv>
         </section>
         <!-- 
           Check Instance Health
           -->
         <section><title outputclass="headerH">Check instance health</title>
           <sectiondiv outputclass="insideSection">
         
         <p>The following procedure checks to see if MySQL is running on an instance, and reboots it if needed:</p>
         <ol>
           <li>Check to see if MySQL is running, and that <codeph>/dev/vdb1</codeph> is mounted as Read-Only (RO).
             <codeblock outputclass="noverticalscroll">stack@eu-em2-cp1-c0-m1-mgmt:~$ ssh ec2-user@10.16.86.130
Linux trove-database0-5bcqh2i3khcu 3.14.29-4-amd64-hlinux #hlinux1 SMP Mon Feb 9 20:32:22 UTC 2015 x86_64
The programs included with the hLinux system are free software; the exact
license terms for each program are described in the individual files in
/usr/share/doc/*/copyright.
hLinux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.
$ uptime
01:44:03 up 35 days, 14:51,  1 user,  load average: 0.00, 0.01, 0.05
$  ps -ef | grep mysql
ec2-user  2255  2224  0 01:29 pts/0    00:00:00 grep mysql
$ grep "\sro[\s,]" /proc/mounts
/dev/vdb1 /mnt ext4 ro,relatime,data=ordered 0 0</codeblock></li>
           <li>If conditions are similar to the above output, the instance needs to be soft rebooted. If so, reboot the instance in Horizon:
           <image placement="break" href="../../../media/dbaas_troubleshooting_reboot_instance.png" /></li>
           <li>Once the reboot is complete, SSH back into the instance and check for MySQL and RO mounts. 
             <codeblock outputclass="noverticalscroll">$ uptime                                
01:28:16 up 18 min,  1 user,  load average: 0.00, 0.01, 0.03
$  ps -ef | grep mysql
ec2-user  2255  2224  0 01:29 pts/0    00:00:00 grep mysql
$ grep "\sro[\s,]" /proc/mounts
$</codeblock></li>
           <li>Fortunately, the reboot has allowed the volume to be mounted with RW privileges (as shown above). Now – to address MySQL:
             <codeblock outputclass="noverticalscroll">$ uptime                                
01:28:16 up 18 min,  1 user,  load average: 0.00, 0.01, 0.03
$  ps -ef | grep mysql
ec2-user  2255  2224  0 01:29 pts/0    00:00:00 grep mysql
$ sudo service mysql bootstrap-pxc
[ ok ] Bootstrapping Percona XtraDB Cluster database server: mysqld ..
$ ps -ef | grep mysql
root      2335     1  0 01:32 pts/0    00:00:00 /bin/sh /usr/bin/mysqld_safe --wsrep-new-cluster
mysql     3017  2335  2 01:32 pts/0    00:00:00 /usr/sbin/mysqld --basedir=/usr --datadir=/mnt/state/var/lib/mysql/ --plugin-dir=/usr/lib/mysql/plugin --user=mysql --wsrep-new-cluster --log-error=/mnt/state/var/log/mysql/error.log --open-files-limit=65535 --pid-file=/var/run/mysqld/mysqld.pid --socket=/var/run/mysqld/mysqld.sock --wsrep_start_position=15b77de3-92f2-11e5-8381-0f7631b33740:248798
ec2-user  3075  2224  0 01:32 pts/0    00:00:00 grep mysql</codeblock></li>
           <li>Now that MySQL is up and running on the first instance, you will need to perform similar steps on the other two instances (see below) – 
             however, DO NOT run sudo service mysql bootstrap-pxc again. ONLY run that command on the first instance you soft reboot. 
             <codeblock outputclass="noverticalscroll">stack@eu-em4-cp1-c0-m1-mgmt:~$ ssh ec2-user@10.16.88.165
Linux trove-database1-gfnobr4vmfug 3.14.29-4-amd64-hlinux #hlinux1 SMP Mon Feb 9 20:32:22 UTC 2015 x86_64
The programs included with the hLinux system are free software; the exact
license terms for each program are described in the individual files in
/usr/share/doc/*/copyright.
hLinux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.
Last login: Tue Jan 26 17:13:44 2016 from 10.16.84.10
$ uptime
02:07:35 up 0 min,  1 user,  load average: 0.23, 0.05, 0.02
$ ps -ef|grep mysql
root      1177     1  0 02:07 ?        00:00:00 /bin/sh /usr/bin/mysqld_safe
mysql     1932  1177 17 02:07 ?        00:00:00 /usr/sbin/mysqld --basedir=/usr --datadir=/mnt/state/var/lib/mysql/ --plugin-dir=/usr/lib/mysql/plugin --user=mysql --log-error=/mnt/state/var/log/mysql/error.log --open-files-limit=65535 --pid-file=/var/run/mysqld/mysqld.pid --socket=/var/run/mysqld/mysqld.sock --wsrep_start_position=32977d8c-a12a-11e5-8c22-af1254754a07:498
ec2-user  2008  1776  0 02:07 pts/1    00:00:00 grep mysql</codeblock></li>
           <li>MySQL will now be running, so there are no other actions to perform aside from rebooting the final database instance, and ensuring <codeph>mysqld_safe</codeph> is running.</li>
         </ol>
           </sectiondiv></section>
         <!-- 
           RabbitMQ Borker Issues
           -->
         
         <section><title outputclass="headerH">RabbitMQ Broker Issues</title>
           <sectiondiv outputclass="insideSection">
         <p>The following section describes how to resolve issues with the RabbitMQ broker, such as when the following error is encountered:</p>
         <codeblock>ERROR trove.guestagent.api MessagingTimeout: Timed out waiting for a reply to message ID 
SecurityGroupCreationError: Connection to neutron failed: __str__ returned non-string
Error creating security group for instance</codeblock>
         <p>In this scenario, the Jenkins Console Output shows that instances are continuously added, but can't be deleted, so each consecutive failure lists an additional instance:</p>
         <image placement="break" href="../../../media/dbaas_troubleshooting_rabbitmq_fail.png" />
         <p>The solution for this scenario is to log into the Rabbitmq Broker for Trove and restore it to a healthy state. The following procedure accomplishes this:</p>
         <ol>
           <li>Log in to each trove-messaging instance from deployer node as user "stack":
             <codeblock>stack@eu-em3-cp1-c0-m1-mgmt:~$ nova list|grep trove-mess
| a4f9ad9c-ab2f-4c77-9c3c-e1070094cfd1 | trove-messaging0-rmq-ry6xrb74vtw6   | ACTIVE | -          | Running     | trove_guest_network=172.18.0.15; SVC=10.16.87.127; trove_mgmt_network=172.17.0.22 |
| 63d41ba7-a262-4a03-a3f4-fe4aef3b4788 | trove-messaging1-rmq-khqp7r5u6tvw   | ACTIVE | -          | Running     | trove_guest_network=172.18.0.14; SVC=10.16.87.130; trove_mgmt_network=172.17.0.23 |
| 76b7f51b-d63f-42e6-ba17-09c6ca719bdc | trove-messaging2-rmq-zqnrd3daljvc   | ACTIVE | -          | Running     | trove_guest_network=172.18.0.13; SVC=10.16.87.125; trove_mgmt_network=172.17.0.24 |</codeblock></li>
           <li>The cluster status of each RabbitMQ instance needs to be checked. First, SSH to the SVC address of each of the returned instances (above).
             <codeblock>stack@eu-em2-cp1-c0-m1-mgmt:~$ ssh ec2-user@10.16.87.127</codeblock>
           <p>If the above command fails, try using the certificate file:</p>
             <codeblock>stack@eu-em2-cp1-c0-m1-mgmt:~$ ssh -i dbaas-key ec2-user@10.16.87.127</codeblock></li>
           <li>Once connected to each instance, run the following commands to check the status:
             <codeblock>$ sudo -s
               # rabbitmqctl cluster_status</codeblock></li>
           <li>Inspect <b>Running Nodes</b> and <b>Cluster Name</b>:  there should be three <b>running_nodes</b> in a healthy cluster. An example of an unhealthy cluster is shown below:
             <codeblock outputclass="noverticalscroll">Linux trove-messaging0-rmq-ry6xrb74vtw6 3.14.29-4-amd64-hlinux #hlinux1 SMP Mon Feb 9 20:32:22 UTC 2015 x86_64
The programs included with the hLinux system are free software; the exact
license terms for each program are described in the individual files in
/usr/share/doc/*/copyright.
hLinux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.
$ sudo -s
# rabbitmqctl cluster_status
Cluster status of node 'rabbit@trove-messaging0-rmq-ry6xrb74vtw6' ...
[{nodes,
    [{disc,
        ['rabbit@trove-messaging0-rmq-ry6xrb74vtw6',
         'rabbit@trove-messaging1-rmq-khqp7r5u6tvw',
         'rabbit@trove-messaging2-rmq-zqnrd3daljvc']}]},
         {running_nodes,<codeph outputclass="codehighlight">['rabbit@trove-messaging0-rmq-ry6xrb74vtw6']</codeph>},
 {cluster_name,&lt;&lt;<codeph outputclass="codehighlight">rabbit@trove-messaging0-rmq-ry6xrb74vtw6.novalocal"</codeph>&gt;&gt;},
 {partitions,
[]</codeblock></li>
           <li>If there is only one running node, check the logs to see if it has been updating. If all three are listed as running, determine which one is the Master and restart the other two. 
             <note>Logs are stored in /mnt/state/var/log/rabbitmq</note>
             <codeblock># ls -ltr /mnt/state/var/log/rabbitmq
total 643112
-rw-r--r-- 1 rabbitmq rabbitmq      1567 Feb 11 04:37 rabbit@trove-messaging0-rmq-ry6xrb74vtw6-sasl.log
-rw-r--r-- 1 rabbitmq rabbitmq 658535237 Feb 17 01:24 <codeph outputclass="codehighlight">rabbit@trove-messaging0-rmq-ry6xrb74vtw6.log</codeph> 
# date 
<codeph outputclass="codehighlight">Wed Feb 17 01:39:50 UTC 2016</codeph> </codeblock>
             In the above example, the <codeph>rabbit@trove-messaging0-rmq-ry6xrb74vtw6.log</codeph> log should show the current time (the name of the log will vary), while the date
           shows that the node is 15 minutes stale.</li>
           <li>If only one node is running, and the log has not been updated (i.e. it is stale as shown above), the RabbitMQ service needs to be restarted. Using the <codeph>cluster_name</codeph> 
             from earlier, rejoin and start the app again:
             <codeblock outputclass="noverticalscroll"># rabbitmqctl stop_app
Stopping node 'rabbit@trove-messaging0-rmq-ry6xrb74vtw6' ...
# rabbitmqctl join_cluster rabbit@trove-messaging0-rmq-ry6xrb74vtw6.novalocal
Clustering node 'rabbit@trove-messaging0-rmq-ry6xrb74vtw6' with 'rabbit@trove-messaging0-rmq-ry6xrb74vtw6.novalocal' ...
Error: unable to connect to nodes ['rabbit@trove-messaging0-rmq-ry6xrb74vtw6.novalocal']: nodedown
DIAGNOSTICS
===========
attempted to contact: ['rabbit@trove-messaging0-rmq-ry6xrb74vtw6.novalocal']
rabbit@trove-messaging0-rmq-ry6xrb74vtw6.novalocal:
  * connected to epmd (port 4369) on trove-messaging0-rmq-ry6xrb74vtw6.novalocal
  * epmd reports node 'rabbit' running on port 5535
  * TCP connection succeeded but Erlang distribution failed
  * suggestion: hostname mismatch?
  * suggestion: is the cookie set correctly?
current node details:
- node name: 'rabbitmqctl-1910@trove-messaging0-rmq-ry6xrb74vtw6'
- home dir: /mnt/state/var/lib/rabbitmq
- cookie hash: V5kCwGFL0o9stIpWdmHQ1Q==</codeblock>
             <note>Expect the "Error: unable to connect to nodes..." as depicted in the code output above. Restarting the app will resolve this issue:
               <codeblock outputclass="noverticalscroll"># rabbitmqctl start_app
Starting node 'rabbit@trove-messaging0-rmq-ry6xrb74vtw6' ...
# rabbitmqctl cluster_status
Cluster status of node 'rabbit@trove-messaging0-rmq-ry6xrb74vtw6' ...
[{nodes,
     [{disc,
          ['rabbit@trove-messaging0-rmq-ry6xrb74vtw6',
          'rabbit@trove-messaging1-rmq-khqp7r5u6tvw',
          'rabbit@trove-messaging2-rmq-zqnrd3daljvc']}]},
 {running_nodes,
     ['rabbit@trove-messaging2-rmq-zqnrd3daljvc',
     'rabbit@trove-messaging1-rmq-khqp7r5u6tvw',
     'rabbit@trove-messaging0-rmq-ry6xrb74vtw6']},
 {cluster_name,&lt;&lt;"rabbit@trove-messaging0-rmq-ry6xrb74vtw6.novalocal"&gt;&gt;},
 {partitions,
     []
#</codeblock></note>
           <note>When the entire cluster is brought down, the last node to go down must be the first node to be brought online.</note>
           <note>If the cluster looks healthy (in that all 3 nodes are running), the Master node needs to be determined. Check the timestamp on the logs in <codeph>/mnt/state/var/log/rabbitmq</codeph></note>
           <note>One of the three nodes will have a more up-to-date log. This one can be presumed to be the current Master. Performing a stop -> join cluster -> start on the other two nodes should correct partition issues.</note></li>
         </ol>
         
         <!-- 
           Restoring a Trove Cluster after one node goes down
           -->
         
           </sectiondiv></section>
         <section><title outputclass="headerH">Restoring a Trove cluster to three nodes when a single node crashes</title>
           <sectiondiv outputclass="insideSection">

         <p>While running a PXC cluster, one of the compute nodes that an instance is running on may go down due to hardware failure or
           other situations. Then, you may only have 2 working nodes in your cluster and could cause a split-brain because you do not have 
           quorum or majority vote.</p>
           
           <p>This section describes how to bring a new instance up and join the cluster if you are running a Liberty version of Trove that does not 
             have the <codeph>cluster-grow</codeph> and <codeph>cluster-shrink</codeph> commands for the PXC datastore.</p>
           
           <p>Some of the details here are specific to a sample environment and must be adopted for your specific environment. This section will describe 
             how to get the information needed to make sure that when you update trove later that this does not cause a problem.</p>
           
             <note>Make a manual backup of the data via <codeph>mysqldump</codeph> before proceeding.</note>
         <p><b>Setting up the environment</b></p>
             <p>For the purposes of this tutorial, you'll deliberately terminate one instance of a Trove cluster in order to demonstrate how to create a new
             instance and add it back to the cluster.</p>
         <p>The following Trove configuration has 3 instances in a PXC cluster. This information is available from the troveclient CLI and from the novaclient
           CLI. This output shows that everything is active and running in a good state. Note the nova ID of the instance you'll delete (test-member-3):</p>
         
         <codeblock outputclass="noverticalscroll">vagrant@packer-ubuntu-1404-server-vagrant:~$ trove cluster-list
+--------------------------------------+------+-----------+-------------------+-----------+
| ID                                   | Name | Datastore | Datastore Version | Task Name |
+--------------------------------------+------+-----------+-------------------+-----------+
| f3153ab0-f433-4373-a462-7d7daac21438 | test | pxc       | 5.5               | NONE      |
+--------------------------------------+------+-----------+-------------------+-----------+
vagrant@packer-ubuntu-1404-server-vagrant:~$ trove cluster-show test
+-------------------+--------------------------------------+
| Property          | Value                                |
+-------------------+--------------------------------------+
| created           | 2016-02-06T16:08:29                  |
| datastore         | pxc                                  |
| datastore_version | 5.5                                  |
| id                | f3153ab0-f433-4373-a462-7d7daac21438 |
| ip                | 10.0.0.6, 10.0.0.7, 10.0.0.9         |
| name              | test                                 |
| task_description  | No tasks for the cluster.            |
| task_name         | NONE                                 |
| updated           | 2016-02-06T16:18:21                  |
+-------------------+--------------------------------------+
vagrant@packer-ubuntu-1404-server-vagrant:~$ trove cluster-instances test
+--------------------------------------+---------------+-----------+------+--------+
| ID                                   | Name          | Flavor ID | Size | Status |
+--------------------------------------+---------------+-----------+------+--------+
| 011ec6fd-13c6-4310-9004-f672be3106c9 | test-member-3 | 7         |    1 | ACTIVE |
| 27df03af-7302-4d12-9dec-5d7c12d66c95 | test-member-2 | 7         |    1 | ACTIVE |
| 9fb77dd1-d37d-4af1-89f0-bb5c5b1e6f55 | test-member-1 | 7         |    1 | ACTIVE |
+--------------------------------------+---------------+-----------+------+--------+
vagrant@packer-ubuntu-1404-server-vagrant:~$ nova list
+--------------------------------------+---------------+--------+------------+-------------+------------------+
| ID                                   | Name          | Status | Task State | Power State | Networks         |
+--------------------------------------+---------------+--------+------------+-------------+------------------+
| <codeph outputclass="codehighlight">dad115c5-bc44-485e-aa60-e080eb2f448e</codeph> | test-member-3 | ACTIVE | -          | Running     | private=10.0.0.9 |
| e491993a-e162-4c41-a15c-80faa4bdd67a | test-member-1 | ACTIVE | -          | Running     | private=10.0.0.6 |
| 2a845f18-c4f3-4583-bba1-2b4eed18d872 | test-member-2 | ACTIVE | -          | Running     | private=10.0.0.7 |
+--------------------------------------+---------------+--------+------------+-------------+------------------+
vagrant@packer-ubuntu-1404-server-vagrant:~$ trove list --include-clustered
+--------------------------------------+---------------+-----------+-------------------+--------+-----------+------+
| ID                                   | Name          | Datastore | Datastore Version | Status | Flavor ID | Size |
+--------------------------------------+---------------+-----------+-------------------+--------+-----------+------+
| 011ec6fd-13c6-4310-9004-f672be3106c9 | test-member-3 | pxc       | 5.5               | ACTIVE | 7         |    1 |
| 27df03af-7302-4d12-9dec-5d7c12d66c95 | test-member-2 | pxc       | 5.5               | ACTIVE | 7         |    1 |
| 9fb77dd1-d37d-4af1-89f0-bb5c5b1e6f55 | test-member-1 | pxc       | 5.5               | ACTIVE | 7         |    1 |
+--------------------------------------+---------------+-----------+-------------------+--------+-----------+------+</codeblock>
         <p><b>Terminating a cluster instance</b></p>
         <p>The following code shows how to kill one of the instances in the cluster from the nova command to
           simulate an instance that is no longer available to the cluster.</p>
             <codeblock outputclass="noverticalscroll">vagrant@packer-ubuntu-1404-server-vagrant:~$ nova delete <codeph outputclass="codehighlight">dad115c5-bc44-485e-aa60-e080eb2f448e</codeph>
Request to delete server dad115c5-bc44-485e-aa60-e080eb2f448e has been accepted.
vagrant@packer-ubuntu-1404-server-vagrant:~$ nova list
+--------------------------------------+---------------+--------+------------+-------------+------------------+
| ID                                   | Name          | Status | Task State | Power State | Networks         |
+--------------------------------------+---------------+--------+------------+-------------+------------------+
| e491993a-e162-4c41-a15c-80faa4bdd67a | test-member-1 | ACTIVE | -          | Running     | private=10.0.0.6 |
| 2a845f18-c4f3-4583-bba1-2b4eed18d872 | test-member-2 | ACTIVE | -          | Running     | private=10.0.0.7 |
 +--------------------------------------+---------------+--------+------------+-------------+------------------+</codeblock>
         <p><b>Creating a new instance</b></p>
         <p>The following procedure demonstrates how to restore the missing instance:</p>
         <ol>
           <li>Get the flavor and volume information for the missing node using the Trove CLI. In the following example,
             you'll get the information for one of the remaining nodes, <b>test-member-1</b>:
             
             <codeblock outputclass="noverticalscroll">vagrant@packer-ubuntu-1404-server-vagrant:~$ trove cluster-instances test
+--------------------------------------+---------------+-----------+------+--------+
| ID                                   | Name          | Flavor ID | Size | Status |
+--------------------------------------+---------------+-----------+------+--------+
| 011ec6fd-13c6-4310-9004-f672be3106c9 | test-member-3 | 7         |    1 | ACTIVE |
| 27df03af-7302-4d12-9dec-5d7c12d66c95 | test-member-2 | 7         |    1 | ACTIVE |
| <codeph outputclass="codehighlight">9fb77dd1-d37d-4af1-89f0-bb5c5b1e6f55</codeph> | test-member-1 | 7         |    1 | ACTIVE |
+--------------------------------------+---------------+-----------+------+--------+
vagrant@packer-ubuntu-1404-server-vagrant:~$ trove show <codeph outputclass="codehighlight">9fb77dd1-d37d-4af1-89f0-bb5c5b1e6f55</codeph>
+-------------------+--------------------------------------+
| Property          | Value                                |
+-------------------+--------------------------------------+
| cluster_id        | f3153ab0-f433-4373-a462-7d7daac21438 |
| created           | 2016-02-06T16:08:29                  |
| datastore         | pxc                                  |
| datastore_version | 5.5                                  |
| flavor            | <codeph outputclass="codehighlight">7</codeph>                                    |
| id                | 9fb77dd1-d37d-4af1-89f0-bb5c5b1e6f55 |
| ip                | 10.0.0.6                             |
| name              | test-member-1                        |
| status            | ACTIVE                               |
| updated           | 2016-02-06T16:08:52                  |
| volume            | <codeph outputclass="codehighlight">1</codeph>                                    |
| volume_used       | 0.24                                 |
+-------------------+--------------------------------------+</codeblock>
           <p>The above output shows that the remaining node has a flavor of <b>7</b> and a volume of <b>1</b>.</p></li>
           <li>Next, we need the networks that the instances in the cluster were set up on. For this, we will use the debug
             feature of the CLI to see the raw JSON that has the <codeph>net_id</codeph> field:
             <codeblock outputclass="noverticalscroll">vagrant@packer-ubuntu-1404-server-vagrant:~$ nova list
+--------------------------------------+---------------+--------+------------+-------------+------------------+
| ID                                   | Name          | Status | Task State | Power State | Networks         |
+--------------------------------------+---------------+--------+------------+-------------+------------------+
| <codeph outputclass="codehighlight">e491993a-e162-4c41-a15c-80faa4bdd67a</codeph> | test-member-1 | ACTIVE | -          | Running     | private=10.0.0.6 |
| 2a845f18-c4f3-4583-bba1-2b4eed18d872 | test-member-2 | ACTIVE | -          | Running     | private=10.0.0.7 |
+--------------------------------------+---------------+--------+------------+-------------+------------------+
vagrant@packer-ubuntu-1404-server-vagrant:~$ nova show <codeph outputclass="codehighlight">e491993a-e162-4c41-a15c-80faa4bdd67a</codeph>
+--------------------------------------+---------------------------------------------------------------------------------+
| Property                             | Value                                                                           |
+--------------------------------------+---------------------------------------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                                                          |
| OS-EXT-AZ:availability_zone          | nova                                                                            |
| OS-EXT-STS:power_state               | 1                                                                               |
...
...
...
DEBUG (session:254) RESP: [200] Content-Length: 158 X-Compute-Request-Id: req-1f2557ad-2808-4110-8420-558a2e60ba13 Vary: X-OpenStack-Nova-API-Version Connection: keep-alive X-Openstack-Nova-Api-Version: 2.12 Date: Sun, 07 Feb 2016 02:52:36 GMT Content-Type: application/json
RESP BODY: {"virtual_interfaces": [{"net_id": "<codeph outputclass="codehighlight">76f575e1-0160-4d8d-9666-1c0be6b2211e</codeph>", "id": "23b4afba-d480-4df9-aaa6-e8995d8fbf06", "mac_address": "fa:16:3e:98:55:d2"}]}

+--------------------------------------+-------------------+
| Id                                   | Mac address       |
+--------------------------------------+-------------------+
| 23b4afba-d480-4df9-aaa6-e8995d8fbf06 | fa:16:3e:98:55:d2 |
+--------------------------------------+-------------------+</codeblock>
             <p>In the above output, the output from the <codeph>nova show</codeph> command includes a <codeph>RESP BODY</codeph> line that shows that the <codeph>net_id</codeph> is <b>76f575e1-0160-4d8d-9666-1c0be6b2211e</b>. </p>
             <note>You can choose which availability zone the new instance should go into, and the name of the instance that you will create. This example uses the same availability zone (nova) as the other instance and
               calls the new instance <b>test-member-3</b>.</note>
           </li>
           <li>Next, create the instance using <codeph>trove create</codeph>:
             <codeblock outputclass="noverticalscroll">vagrant@packer-ubuntu-1404-server-vagrant:~$ trove create test-member-3 7 --size 1 --datastore pxc --nic net-id=<codeph outputclass="codehighlight">76f575e1-0160-4d8d-9666-1c0be6b2211e</codeph> --availability_zone nova
+-------------------+--------------------------------------+
| Property          | Value                                |
+-------------------+--------------------------------------+
| created           | 2016-02-07T03:06:53                  |
| datastore         | pxc                                  |
| datastore_version | 5.5                                  |
| flavor            | 7                                    |
| id                | <codeph outputclass="codehighlight">42d4589f-c254-492b-94e2-a98f111b8249</codeph> |
| name              | test-member-3                        |
| status            | BUILD                                |
| updated           | 2016-02-07T03:06:53                  |
| volume            | 1                                    |
+-------------------+--------------------------------------+</codeblock></li>
           <li>Next, wait for the instance to go active. Use <codeph>trove show</codeph> to check the instance status:
             <codeblock outputclass="noverticalscroll">vagrant@packer-ubuntu-1404-server-vagrant:~$ trove show <codeph outputclass="codehighlight">42d4589f-c254-492b-94e2-a98f111b8249</codeph>
+-------------------+--------------------------------------+
| Property          | Value                                |
+-------------------+--------------------------------------+
| created           | 2016-02-07T03:06:53                  |
| datastore         | pxc                                  |
| datastore_version | 5.5                                  |
| flavor            | 7                                    |
| id                | 42d4589f-c254-492b-94e2-a98f111b8249 |
| ip                | 10.0.0.10                            |
| name              | test-member-3                        |
| status            | <codeph outputclass="codehighlight">ACTIVE</codeph>                               |
| updated           | 2016-02-07T03:07:17                  |
| volume            | 1                                    |
| volume_used       | 0.1                                  |
+-------------------+--------------------------------------+
vagrant@packer-ubuntu-1404-server-vagrant:~$ nova list
+--------------------------------------+---------------+--------+------------+-------------+-------------------+
| ID                                   | Name          | Status | Task State | Power State | Networks          |
+--------------------------------------+---------------+--------+------------+-------------+-------------------+
| e491993a-e162-4c41-a15c-80faa4bdd67a | test-member-1 | ACTIVE | -          | Running     | private=10.0.0.6  |
| 2a845f18-c4f3-4583-bba1-2b4eed18d872 | test-member-2 | ACTIVE | -          | Running     | private=10.0.0.7  |
| <codeph outputclass="codehighlight">87936718-ca69-4850-aed5-99739a06fdf9</codeph> | test-member-3 | ACTIVE | -          | Running     | private=10.0.0.10 |
+--------------------------------------+---------------+--------+------------+-------------+-------------------+
vagrant@packer-ubuntu-1404-server-vagrant:~$ nova show <codeph outputclass="codehighlight">87936718-ca69-4850-aed5-99739a06fdf9</codeph>
+--------------------------------------+---------------------------------------------------------------------------------+
| Property                             | Value                                                                           |
+--------------------------------------+---------------------------------------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                                                          |
| OS-EXT-AZ:availability_zone          | nova                                                                            |
| OS-EXT-STS:power_state               | 1                                                                               |
| OS-EXT-STS:task_state                | -                                                                               |
| OS-EXT-STS:vm_state                  | active                                                                          |
| OS-SRV-USG:launched_at               | 2016-02-07T03:07:13.000000                                                      |
| OS-SRV-USG:terminated_at             | -                                                                               |
| accessIPv4                           |                                                                                 |
| accessIPv6                           |                                                                                 |
| config_drive                         | True                                                                            |
| created                              | 2016-02-07T03:07:01Z                                                            |
| flavor                               | m1.rd-tiny (7)                                                                  |
| hostId                               | 3cb1f16704c8010c86d77ca569304e3d47c274294cdf4c312b29ca27                        |
| id                                   | 87936718-ca69-4850-aed5-99739a06fdf9                                            |
| image                                | ubuntu_pxc (4e3ce66b-829e-4558-8c25-64f512491e3a)                               |
| key_name                             | -                                                                               |
| locked                               | False                                                                           |
| metadata                             | {}                                                                              |
| name                                 | test-member-3                                                                   |
| os-extended-volumes:volumes_attached | [{"id": "b5fafe00-50a2-4a66-9c1d-e44f2283e234", "delete_on_termination": true}] |
| private network                      | 10.0.0.10                                                                       |
| progress                             | 0                                                                               |
| security_groups                      | SecGroup_42d4589f-c254-492b-94e2-a98f111b8249                                   |
| status                               | <codeph outputclass="codehighlight">ACTIVE</codeph>                                                                          |
| tenant_id                            | 3e45bf991dcc4e1c9dad511ddcc622e6                                                |
| updated                              | 2016-02-07T03:07:13Z                                                            |
| user_id                              | 59846cd6eaaa4fbe9afbca861492fbff                                                |
+--------------------------------------+---------------------------------------------------------------------------------+</codeblock></li>
           <li>Next, hook up the new instance to the cluster. This will involve connecting the PXC service running on the new instance to the
             existing instances in the cluster, and set up the new associations of the new instance in the infrastructure trove database:
             <p>Connecting the PXC service on the new instance to the existing instances</p>
           <ol>
             <li>Log in to the new instance and stop the MySql service:
               <codeblock>$ sudo service mysql stop</codeblock></li>
             <li>Copy the following files from a remaining instance (<b>test-instance-1</b> in this example) to the new instance (<b>test-member-3</b> in this example):
               <ul><li><b>/etc/mysql/conf.d/50-system-001-common.cnf</b></li>
                 <li><b>/etc/mysql/conf.d/50-system-002-cluster.cnf</b></li></ul></li>
             <li>Make the following changes to the <b>50-system-002-cluster.cnf</b> file:
             <ul>
               <li>Update the <b>wsrep_node_address</b> to the ip of the current system.</li>
               <li>The <b>wsrep_node_name</b> should match the name of the new instance (<b>test-member-3</b> in this example.)</li>
               <li>The <b>wsrep_cluster_address</b> should match the list of ip addresses that are in the
                 cluster.</li>
             </ul></li>
             <li>Start the MySql service on the new instance:
               <codeblock>$ sudo service mysql start</codeblock></li>
           </ol>
             <note>Starting the MySql service may take some time for the new instance to be synchronized with
               the existing cluster, depending on how large the dataset is.</note></li>
           <li>Check the MySql database status:
             <codeblock outputclass="noverticalscroll">$ mysql
mysql> show status like 'wsrep%';
+----------------------------+--------------------------------------------+
| Variable_name              | Value                                      |
+----------------------------+--------------------------------------------+
| wsrep_local_state_uuid     | 43057407-ccec-11e5-919d-4a31a05f51d6       |
| wsrep_protocol_version     | 4                                          |
| wsrep_last_committed       | 0                                          |
| wsrep_replicated           | 0                                          |
| wsrep_replicated_bytes     | 0                                          |
| wsrep_received             | 3                                          |
| wsrep_received_bytes       | 276                                        |
| wsrep_local_commits        | 0                                          |
| wsrep_local_cert_failures  | 0                                          |
| wsrep_local_replays        | 0                                          |
| wsrep_local_send_queue     | 0                                          |
| wsrep_local_send_queue_avg | 0.000000                                   |
| wsrep_local_recv_queue     | 0                                          |
| wsrep_local_recv_queue_avg | 0.000000                                   |
| wsrep_flow_control_paused  | 0.000000                                   |
| wsrep_flow_control_sent    | 0                                          |
| wsrep_flow_control_recv    | 0                                          |
| wsrep_cert_deps_distance   | 0.000000                                   |
| wsrep_apply_oooe           | 0.000000                                   |
| wsrep_apply_oool           | 0.000000                                   |
| wsrep_apply_window         | 0.000000                                   |
| wsrep_commit_oooe          | 0.000000                                   |
| wsrep_commit_oool          | 0.000000                                   |
| wsrep_commit_window        | 0.000000                                   |
| wsrep_local_state          | 4                                          |
| wsrep_local_state_comment  | Synced                                     |
| wsrep_cert_index_size      | 0                                          |
| wsrep_causal_reads         | 0                                          |
| wsrep_incoming_addresses   | 10.0.0.7:3306,10.0.0.6:3306,10.0.0.10:3306 |
| wsrep_cluster_conf_id      | 7                                          |
| wsrep_cluster_size         | 3                                          |
| wsrep_cluster_state_uuid   | 43057407-ccec-11e5-919d-4a31a05f51d6       |
| wsrep_cluster_status       | Primary                                    |
| wsrep_connected            | ON                                         |
| wsrep_local_bf_aborts      | 0                                          |
| wsrep_local_index          | 2                                          |
| wsrep_provider_name        | Galera                                     |
| wsrep_provider_vendor      | Codership Oy &lt;info@codership.com&gt;          |
| wsrep_provider_version     | 2.8(r165)                                  |
| wsrep_ready                | <codeph outputclass="codehighlight">ON</codeph>                                         |
+----------------------------+--------------------------------------------+</codeblock></li>
         </ol>
             
             <p><b>Associating the instances in the cluster with the new instance</b></p>
             
         <p>Now that the new instance has been created and added to the cluster, we need to tell the other instances in the cluster that if their MySql services restart, they need to notify the new instance. To create
         this association, do the following:</p>
         <ol>
           <li>Get the IP address of the new instance (<b>test-member-3</b> in this example):
             <codeblock>vagrant@packer-ubuntu-1404-server-vagrant:~$ nova list
+--------------------------------------+---------------+--------+------------+-------------+-------------------+
| ID                                   | Name          | Status | Task State | Power State | Networks          |
+--------------------------------------+---------------+--------+------------+-------------+-------------------+
| e491993a-e162-4c41-a15c-80faa4bdd67a | test-member-1 | ACTIVE | -          | Running     | private=10.0.0.6  |
| 2a845f18-c4f3-4583-bba1-2b4eed18d872 | test-member-2 | ACTIVE | -          | Running     | private=10.0.0.7  |
| 87936718-ca69-4850-aed5-99739a06fdf9 | test-member-3 | ACTIVE | -          | Running     | private=<codeph outputclass="codehighlight">10.0.0.10</codeph> |
+--------------------------------------+---------------+--------+------------+-------------+-------------------+</codeblock></li>
           <li>On the existing instances (<b>test-member-1</b> and <b>test-member-2</b> in this example), edit the <b>/etc/mysql/conf.d/50-system-002-cluster.cnf</b> file with the IP address
           of the new instance (<b>test-member-3</b> in this example). The <b>wsrep_cluster_address</b> line should look like the following:
             <codeblock>wsrep_cluster_address = "gcomm://10.0.0.7,10.0.0.6<codeph outputclass="codehighlight">,10.0.0.10</codeph>"</codeblock></li>
         </ol>
         <note>The MySql service does not need to be restarted to register this update.</note>
             <p><b>Remove the old instance from the cluster, and add the new instance</b></p>
         <p>Lastly, you need to remove the old instance from the Trove infrastructure database, and add the new instance:</p>
         <ol>
           <li>Find the ID of the cluster, and the ID for the old and new instances:
           <codeblock>vagrant@packer-ubuntu-1404-server-vagrant:~$ trove cluster-list
+--------------------------------------+------+-----------+-------------------+-----------+
| ID                                   | Name | Datastore | Datastore Version | Task Name |
+--------------------------------------+------+-----------+-------------------+-----------+
| <codeph outputclass="codehighlight">f3153ab0-f433-4373-a462-7d7daac21438</codeph> | test | pxc       | 5.5               | NONE      |
+--------------------------------------+------+-----------+-------------------+-----------+
vagrant@packer-ubuntu-1404-server-vagrant:~$ trove list
+--------------------------------------+---------------+-----------+-------------------+--------+-----------+------+
| ID                                   | Name          | Datastore | Datastore Version | Status | Flavor ID | Size |
+--------------------------------------+---------------+-----------+-------------------+--------+-----------+------+
| <codeph outputclass="codehighlight">42d4589f-c254-492b-94e2-a98f111b8249</codeph> | test-member-3 | pxc       | 5.5               | ACTIVE | 7         |    1 |
+--------------------------------------+---------------+-----------+-------------------+--------+-----------+------+
vagrant@packer-ubuntu-1404-server-vagrant:~$ trove cluster-instances test
+--------------------------------------+---------------+-----------+------+--------+
| ID                                   | Name          | Flavor ID | Size | Status |
+--------------------------------------+---------------+-----------+------+--------+
| <codeph outputclass="codehighlight">011ec6fd-13c6-4310-9004-f672be3106c9</codeph> | test-member-3 | 7         |    1 | ACTIVE |
| 27df03af-7302-4d12-9dec-5d7c12d66c95 | test-member-2 | 7         |    1 | ACTIVE |
| 9fb77dd1-d37d-4af1-89f0-bb5c5b1e6f55 | test-member-1 | 7         |    1 | ACTIVE |
+--------------------------------------+---------------+-----------+------+--------+
vagrant@packer-ubuntu-1404-server-vagrant:~$ nova list
+--------------------------------------+---------------+--------+------------+-------------+-------------------+
| ID                                   | Name          | Status | Task State | Power State | Networks          |
+--------------------------------------+---------------+--------+------------+-------------+-------------------+
| e491993a-e162-4c41-a15c-80faa4bdd67a | test-member-1 | ACTIVE | -          | Running     | private=10.0.0.6  |
| 2a845f18-c4f3-4583-bba1-2b4eed18d872 | test-member-2 | ACTIVE | -          | Running     | private=10.0.0.7  |
| 87936718-ca69-4850-aed5-99739a06fdf9 | test-member-3 | ACTIVE | -          | Running     | private=10.0.0.10 |
+--------------------------------------+---------------+--------+------------+-------------+-------------------+</codeblock>
             In the above output, the cluster ID is <b>f3153ab0-f433-4373-a462-7d7daac21438</b>; the old instance ID is <b>011ec6fd-13c6-4310-9004-f672be3106c9</b>, and 
             the new instance ID is <b>42d4589f-c254-492b-94e2-a98f111b8249</b>.</li>
           <li>Launch the MySql Trove CLI:
             
<codeblock outputclass="noverticalscroll">vagrant@packer-ubuntu-1404-server-vagrant:~$ mysql trove
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A
             
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 1221
Server version: 5.5.47-0ubuntu0.14.04.1 (Ubuntu)
             
Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.
             
Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.
             
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.</codeblock></li>
           <li>In the MySql Trove utilty, check the <b>cluster_id</b> of the old instance, using the ID listed above (<b>011ec6fd-13c6-4310-9004-f672be3106c9</b>):
             
             <codeblock outputclass="noverticalscroll">mysql&gt; select * from instances where id='<codeph outputclass="codehighlight">011ec6fd-13c6-4310-9004-f672be3106c9</codeph>'\G
*************************** 1. row ***************************
id: 011ec6fd-13c6-4310-9004-f672be3106c9
created: 2016-02-06 16:15:45
updated: 2016-02-06 16:16:08
...
...
slave_of_id: NULL
cluster_id: <codeph outputclass="codehighlight">f3153ab0-f433-4373-a462-7d7daac21438</codeph>
shard_id: NULL
type: member
1 row in set (0.01 sec)</codeblock></li>
<li>Set the <b>cluster_id</b> of the old instance to <b>NULL</b>:             
  <codeblock>mysql&gt; update instances set cluster_id=NULL where id='<codeph outputclass="codehighlight">011ec6fd-13c6-4310-9004-f672be3106c9</codeph>'\G
Query OK, 1 row affected (0.03 sec)
Rows matched: 1  Changed: 1  Warnings: 0</codeblock></li>
<li>Verify that the <b>cluster_id</b> for the old instance is now <b>NULL</b>:             
  <codeblock outputclass="noverticalscroll">mysql&gt; select * from instances where id='<codeph outputclass="codehighlight">011ec6fd-13c6-4310-9004-f672be3106c9</codeph>'\G
*************************** 1. row ***************************
id: 011ec6fd-13c6-4310-9004-f672be3106c9
created: 2016-02-06 16:15:45
updated: 2016-02-06 16:16:08
...
...
slave_of_id: NULL
<codeph outputclass="codehighlight">cluster_id: NULL</codeph>
shard_id: NULL
type: member
1 row in set (0.00 sec)</codeblock></li>
           <li>Set the <b>cluster_id</b> of the new instance to the ID of the cluster you discovered above:
             <codeblock>mysql&lt; update instances set cluster_id='<codeph outputclass="codehighlight">f3153ab0-f433-4373-a462-7d7daac21438</codeph>', type='member' where id='<codeph outputclass="codehighlight">42d4589f-c254-492b-94e2-a98f111b8249</codeph>'\G
Query OK, 1 row affected (0.02 sec)
Rows matched: 1  Changed: 1  Warnings: 0</codeblock></li>
   <li>Exit the MySql Trove CLI:          
<codeblock>mysql&gt; Bye</codeblock></li>
<li>Verify that the cluster includes the new instance:             
<codeblock>vagrant@packer-ubuntu-1404-server-vagrant:~$ trove cluster-instances test
+--------------------------------------+---------------+-----------+------+--------+
| ID                                   | Name          | Flavor ID | Size | Status |
+--------------------------------------+---------------+-----------+------+--------+
| 27df03af-7302-4d12-9dec-5d7c12d66c95 | test-member-2 | 7         |    1 | ACTIVE |
| <codeph outputclass="codehighlight">42d4589f-c254-492b-94e2-a98f111b8249</codeph> | test-member-3 | 7         |    1 | ACTIVE |
| 9fb77dd1-d37d-4af1-89f0-bb5c5b1e6f55 | test-member-1 | 7         |    1 | ACTIVE |
+--------------------------------------+---------------+-----------+------+--------+</codeblock></li>
           <li>Delete the old instance from the cluster:
             <codeblock>vagrant@packer-ubuntu-1404-server-vagrant:~$ trove list
+--------------------------------------+---------------+-----------+-------------------+--------+-----------+------+
| ID                                   | Name          | Datastore | Datastore Version | Status | Flavor ID | Size |
+--------------------------------------+---------------+-----------+-------------------+--------+-----------+------+
| <codeph outputclass="codehighlight">011ec6fd-13c6-4310-9004-f672be3106c9</codeph> | test-member-1 | pxc       | 5.5               | ACTIVE | 7         |    1 |
+--------------------------------------+---------------+-----------+-------------------+--------+-----------+------+
               vagrant@packer-ubuntu-1404-server-vagrant:~$ trove delete 011ec6fd-13c6-4310-9004-f672be3106c9</codeblock></li>
           <li>Verify that the old instance was deleted:
               <codeblock>vagrant@packer-ubuntu-1404-server-vagrant:~$ trove list
+----+------+-----------+-------------------+--------+-----------+------+
| ID | Name | Datastore | Datastore Version | Status | Flavor ID | Size |
+----+------+-----------+-------------------+--------+-----------+------+
+----+------+-----------+-------------------+--------+-----------+------+</codeblock></li>
         </ol>
           <p><b>Verify the cluster</b></p>
           <p>To verify the cluster, run the commands at the beginning of this troubleshooting topic, and verify that the output is similar to the output given.</p></sectiondiv></section>
         </body>
     </topic>

     
  </topic>
