<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_rgp_g5p_45">
  <title>After Upgrading</title>
  <body>
    
    <section>
      <title>Application Lifecycle Service</title>
      <p>If any ALS clusters do not restart after the upgrade is completed, perform the following
        steps.</p>
      <ol id="ol_eph_xxg_45">
        <li> Start core node.</li>
        <li> Start secondary controller nodes.</li>
        <li> Start secondary routers.</li>
        <li> Start service broker nodes.</li>
        <li> Start DEAs.</li>
      </ol>
    </section>
    
    <section>
      <title>Database Service</title><p>The database services and instances must be restarted after upgrading.</p>
      
      <p><ol id="ol_n1n_v4h_45">
        <li>Restart all control plane
          instances.<codeblock>./control_plane_upgrade/scripts/start_control_plane_instances.bash </codeblock></li>
        <li>Safely start the Trove
          services.<codeblock>ansible-playbook --private-key &lt;path to SSH private key> -i ./common_upgrade/inventory/inventory.py ./control_plane_upgrade/playbooks/control_plane_start.yml</codeblock><note>If
            any of the services do not restart [FAILED = 1], add floating IPs to all the instances
            in the Database control plane and run this playbook again. These instances are created
            during installation and can be accessed through the Horizon dashboard or Nova
            client.</note></li>
        <li>Start all database
          instances.<codeblock>ansible-playbook --extra-vars guest_private_key_file=&lt;path to SSH private key> --private-key &lt;path to SSH private key> -i ./common_upgrade/inventory/inventory.py ./guest-upgrade/playbooks/start-guests.yml </codeblock></li>
        <li>Verify that all guest instances were restarted successfully.
          <codeblock>cat ~/guest-instance-status.log </codeblock></li>
        <li>Restart the Database API service. After this step the database service will be fully
          functional
          <codeblock>ansible-playbook --private-key &lt;path to SSH private key> -i ./common_upgrade/inventory/inventory.py ./control_plane_upgrade/playbooks/start-api.yml</codeblock></li>
      </ol></p></section>
    
    
    <section>
      <title>HPE Helion Code Engine</title>
    </section>
    
    
    
    <section><title>Messaging Broker Service</title>
      <p> Bring up the messaging control plane.</p>
      <ol id="ol_srh_xxg_45">
        <li>SSH into the HPE Helion OpenStack lifecycle manager.
          <codeblock>ssh stack@&lt;hos-deployer-ip></codeblock></li>
        <li>From the HPE Helion OpenStack lifecycle manager, SSH into the Messaging deployer node.
          This was <xref href="installation/devplatform_install_messaging.dita">created during
            installation</xref> of the Messaging
          service.<codeblock>ssh stack@&lt;messaging-deployer-ip></codeblock></li>
        <li>Navigate to the <codeph>ansible</codeph> scratch directory.
          <codeblock>cd scratch/ansible/next/hos/ansible/</codeblock></li>
        <li>Run the following playbooks in this order.
          <codeblock>ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml
ansible-playbook -i hosts/verb_hosts percona-bootstrap.yml
ansible-playbook -i hosts/verb_hosts cue-stop.yml
ansible-playbook -i hosts/verb_hosts cue-start.yml</codeblock></li>
      </ol>
      <p>Bring up tenant RabbitMQ Clusters.</p>
      
      <note>If you did not previously deploy any RabbitMQ clusters, this step does not apply to your
        environment.</note>
      <ol id="ol_r5h_xxg_45">
        <li>SSH into the HPE Helion OpenStack lifecycle manager.
          <codeblock>ssh stack@&lt;hos-deployer-ip></codeblock></li>
        <li>From the HPE Helion OpenStack lifecycle manager, SSH into the Messaging deployer node.
          This was <xref href="installation/devplatform_install_messaging.dita">created during
            installation</xref> of the Messaging
          service.<codeblock>ssh stack@&lt;messaging-deployer-ip></codeblock></li>
        <li>Navigate to the <codeph>ansible</codeph> scratch directory.
          <codeblock>cd scratch/ansible/next/hos/ansible/</codeblock></li>
        <li>Run the RabbitMQ cluster recovery script.<ol id="ol_npz_5zs_35">
          <li>Copy <xref href="installation/cue_rabbit_fix_script.dita">this codeblock</xref> into
            a plaintext file and save it as <codeph>cue-rabbit-fix.sh</codeph></li>
          <li>Copy the script file into this directory on the messaging
            deployer.<codeblock> /home/stack/scratch/ansible/next/hos/ansible</codeblock></li>
          <li>Make the script executable. <codeblock>chmod +x cue-rabbit-fix.sh</codeblock></li>
          <li>Execute the script to bring all your RabbitMQ clusters
            online.<codeblock>./cue-rabbit-fix.sh</codeblock></li>
        </ol></li>
        <li>Verify Installation <ol id="ol_axh_xxg_45">
          <li>Set endpoint type to <codeph>internal</codeph>. (The public endpoint cannot be
            reached from the messaging deployer)
            <codeblock>export OS_ENDPOINT_TYPE=internalURL
export OS_INTERFACE=internal</codeblock></li>
          <li>List all clusters. <note>Since you have not yet created any clusters, this list of
            clusters will be empty, but it should execute without returning any error messages.
          </note><codeblock>stack@messaging-deployer:~/scratch/ansible/next/hos/ansible$ /opt/stack/service/cueclient/venv/bin/openstack message-broker cluster list</codeblock></li>
          <li>If you have configured TLS, make sure that the Cue public endpoint begins with https
            and not just http:<codeblock>openstack catalog list</codeblock></li>
        </ol></li>
      </ol>
    </section>
    
    
  </body>
</topic>
